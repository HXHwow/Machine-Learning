[TOC]

### 线性回归算法原理推导

#### 线性回归算法概述

![1](C:\Users\Hu Xinhua\OneDrive\学习笔记\机器学习笔记\数据分析与机器学习实战\线性回归\1.png)

![2](C:\Users\Hu Xinhua\OneDrive\学习笔记\机器学习笔记\数据分析与机器学习实战\线性回归\2.png)

​		假设$\theta_1$是年龄参数，$\theta_2$ 是工资参数

​		拟合的平面：$h_\theta (x)=\theta_0+\theta_1x_1+\theta_2x_2$ ($\theta_0$ 是偏置项) 

​		整合：
$$
h_{\theta}\left( x \right) =\sum_{i=0}^n{\theta _ix_i}=\theta ^Tx
$$
​	特征的个数增多，也就是权重参数对应的个数变多。

​	$x_0$ 那列对应1，通常都会在第一列增加1。

​	矩阵比较高效，标准格式就是矩阵。

#### 误差项分析

​	**误差** 

​		真实值和预测值之间肯定是存在差异的（用$\varepsilon $ 来表示该误差）

​		对于每个样本：$y^{\left( i \right)}=\theta ^Tx^{\left( i \right)}+\varepsilon ^{\left( i \right)}$

​		**误差$\varepsilon ^{\left( i \right)}$ 是独立并且具有相同的分布，并且服从均值为0方差为$\sigma ^2$ 的高斯分布** 

​		**独立：张三和李四一起来贷款，他两没关系**

​		**同分布：他两都来得是我们假定的这家银行** 

​		**高斯分布：银行可能会多给，也可能会少给，这个浮动不会太大，极小情况下浮动会比较大** 

​		 						![4](C:\Users\Hu Xinhua\OneDrive\学习笔记\机器学习笔记\数据分析与机器学习实战\线性回归\4.png)

​	**独立**： 银行借给张三多少钱和银行借给李四多少钱是没有关系的，不会因为借给张三的钱少就借给李四的钱多，每个人都根据各自的指标进行评估，每条数据都是独立的，银行指的是我们算法，我们的算法会同等的对待每个样本，每个样本之间都是没有关系的。

​	**同分布** ：算法的执行是有一定规则的，我们现在假设进行银行贷款的预测，张三去的是建设银行，李四去的是农业银行，把两个不同的银行的数据放在一起预测，肯定是不符合实际逻辑的，我们建立模型，要么给建设银行建立他的模型，要么给农业银行建立模型，所以我们现在讨论问题一定是同分布的，张三李四都来建设银行，而不是不同的银行。我们的样本数据都要来自同一个地方，符合同一个分布。

​	**高斯分布** ：均值为0方差为$\theta ^2$ 的高斯分布，如上图所示。特点就是中间均值等于0，标准差等于$\theta^2$ ，  为什么假设是这样的一种分布呢？我们做任何问题的时候都进行了一个这样的假设，都是假设是高斯分布，因为在实际情况中，不可能这家银行永远多给你一万块钱，永远没有少贷你钱这一说，很多情况下，多贷和少贷的比例是差不多的。在这里就是，我们认为误差的分布服从一个高斯分布。高斯分布就是正态分布，正态分布就是大致情况下是这个样子。68%的值都在-1到+1的区间范围内，也就是小范围内的偏差是正常的，取到大范围偏差的可能性比较低。

​	误差属于高斯分布的假设是没有严格证明的，因为你永远都没办法确定样本就是独立的，就是同分布的，样本误差就是标准的高斯分布的。但是在机器学习中，它是从实际情况下出发，基于我的实际情况，我做了这样一种假设，误差都是小范围的，极少情况误差大。

​	根据一种假设进行推导？这种方法科学吗？虽然我们是做的一个假设，但是我们最终得到的结果是可利用的，只要这个模型可用，我们就说这个假设是成立的，绝对是没有完完全全的独立，也没有完完全全的分布。只要数据基本符合我们的规则，就是可以的。

#### 似然函数求解

​		**预测值与误差：$y^{\left( i \right)}=\theta ^Tx^{\left( i \right)}+\varepsilon ^{\left( i \right)}$** 

​		**由于误差服从高斯分布：$p\left( \varepsilon ^{\left( i \right)} \right) =\frac{1}{\sqrt{2\pi}\sigma}\exp \left( \frac{-\left( \epsilon ^{\left( i \right)} \right) ^2}{2\sigma ^2} \right) $ ** 

​		**将（1）式代入（2）式：$p\left( y^{\left( i \right)}|x^{\left( i \right)};\theta \right) =\frac{1}{\sqrt{2\pi}\sigma}\exp \left( \frac{-\left( y^{\left( i \right)}-\theta ^Tx^{\left( i \right)} \right) ^2}{2\sigma ^2} \right) $ ** 

​	首先我们提出预测值和我们误差之间的关系，误差服从我们的高斯分布，所以可以写出高斯分布的公式，也就是误差分布服从我们假设的高斯分布。我们把误差项用我们预测值和真实值之间的差异进行表示，所以我们得到第三个式子。

​	接下来，我们要看的就是似然函数。

​	**似然函数：$L\left( \theta \right) =\underset{i=1}{\overset{m}{\varPi}}p\left( y^{\left( i \right)}|x^{\left( i \right)};\theta \right) =\underset{i=1}{\overset{m}{\varPi}}\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( y^{\left( i \right)}-\theta ^Tx^{\left( i \right)} \right) ^2}{2\sigma ^2} \right) $ ** 

​		**解释：什么样的参数跟我们的数据组合后恰好是真实值**

​	**对数似然：$$ \log L\left( \theta \right) =\log \underset{i=1}{\overset{m}{\varPi}}\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( y^{\left( i \right)}-\theta ^Tx^{\left( i \right)} \right) ^2}{2\sigma ^2} \right) $$ **

​		**解释：乘法难解，加法就容易了**

​	似然函数是，比如我现在去赌场赌博，但是我心里没底，我不知道我是赢还是输，我的输赢可能是服从赌场的某种规则，这个规则我自己是不知道的，这个时候该怎么办呢？这个时候我就去赌场蹲一蹲，挨个问哥们是否挣钱，一共有9个人告诉我挣钱了，1个人告诉我没挣钱，那我就觉得有90%的可能性挣钱，我认为我跟他们是一样的，所以我大概率认为自己会挣钱，这个就是我的一个似然函数。

​	似然函数是根据你的样本来估计你的参数值的，这个就是我们的参数估计，当我不知道什么样的参数，比如说赌场服从什么样的参数，我就去看一看我观测的数据，用数据找出来对应的规则，这个就叫似然函数，说白了，由数据去推导参数，也就是什么样的参数跟我们的数据组合之后能更近我们的真实值。

​	让我们预测的值是真实值的可能性越大越好，这就是我的目标，让我的参数跟我的数据组合后，让他成为真实值的可能性越高越好。为什么希望概率越大越好，因为希望越接近真实值越好。

​	最大似然函数和极大似然估计就是：什么样的概率能使这个结果越大越好？就是依照着我的似然函数，依照着我的最大似然估计，由数据估计出来什么参数是合理的。

#### 目标函数推导

​	什么叫做对数似然呢？我们先说这里为什么进行累乘，i从1到m，也就是说我会考虑所有的样本。这里我引入了一个对数似然，那什么是对数似然呢？在机器学习中，加法问题容易求解，但是乘法问题就太难太难了，所以这里我们要引入对数似然，对数似然就是将相乘转化为相加。

​		**误差**

​		**展开化简：** 
$$
\sum_{i=1}^m{\log \frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( y^{\left( i \right)}-\theta ^Tx^{\left( i \right)} \right) ^2}{2\sigma ^2} \right)}
\\
=m\log \frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma ^2}\cdot \frac{1}{2}\sum_{i=1}^m{\left( y^{\left( i \right)}-\theta ^Tx^{\left( i \right)} \right) ^2}
$$
​		**目标：让似然函数（对数变换后也一样）越大越好**

$$
J\left( \theta \right) =\frac{1}{2}\sum_{i=1}^m{\left( y^{\left( i \right)}-\theta ^Tx^{\left( i \right)} \right)}^2（最小二乘法）
$$
​	单纯看图像及函数值，离0越近，这个值越高，所以也就是最大值。

​	**注意1：为什么要用似然函数？** 

​			基于误差服从高斯分布的假设，从已有的数据去推导参数，让它尽可能的和真实值相近。

​	**注意2：为什么要用对数似然？** 

​			相乘在机器学习中比较难算，利用对数似然将其转换为求和问题。

  	**注意3：为什么这个公式的值要越小越好？** 

​			整个值要越大越好，前面的公式是常数，所以后面的值要越小越好。

​	**注意4：为什么似然函数越大越好？** 

​			让预测值成为真实值的可能性越高越好。

#### 线性回归求解

$$
\text{求偏导：}\nabla J\left( \theta \right) =\nabla \left( \frac{1}{2}\left( X\theta -Y \right) ^T\left( X\theta -Y \right) \right) 
\\
\nabla J\left( \theta \right) =\nabla \left( \frac{1}{2}\left( \theta ^TX^T-Y^T \right) \left( X\theta -Y \right) \right) 
\\
\nabla J\left( \theta \right) =\nabla \left( \frac{1}{2}\left( \theta ^TX^TX\theta -\theta ^TX^TY-Y^TX\theta +Y^TY \right) \right) 
\\
\nabla J\left( \theta \right) =\frac{1}{2}\left( 2X^TX\theta -X^TY-\left( Y^TX \right) ^T \right) 
\\
\nabla J\left( \theta \right) =X^TX\theta -X^TY
\\
\text{偏导等于0：}X^TX\theta -X^TY=0
\\
\theta =\left( X^TX \right) ^{-1}X^TY
$$

​	为什么我们求出来的偏导数是极小值点呢？机器学习中有个东西叫凸优化，普遍情况下我们认为我们的函数是凸函数，所以我们认为我们求出来的点是极小值点，这就是我们为什么经常把求最大值的问题还要转化成求最小值的问题，经常求偏导求出来的都是极小值。

​	通过求方程的值求解$\theta$ 的值，但是这种情况并不一定都成立，矩阵求逆是有条件的。

​	机器学习中，基本的思想是优化的思想，不是求解的思想，所以有梯度下降。

​		**评估方法** 

​				**最常用的评估项$R^2：$ ** 
$$
1-\frac{\sum_{i=1}^m{\left( \hat{y}_i-y_i \right) ^2}}{\sum_{i=1}^m{\left( y_i-\bar{y} \right)}^2}
$$
​		**上面是残差平方和，下面是类似方程项**

​		**$R^2$ 的取值越接近1我们认为模型拟合得越好**

​	如果误差不服从高斯分布，我们可以对数据做**log** 变换，为了让数据服从标准分布。原始数据可能存在一些偏布，我们对数据做一些处理，将其转化成正常分布。

​	